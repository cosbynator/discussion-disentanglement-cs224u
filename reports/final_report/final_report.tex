\documentclass[10pt]{article}
\usepackage{acl2012}
%\usepackage[margin=0.75in]{geometry} if we are desperate for more space
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}

\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.0cm}    % Expanding the titlebox

\newcommand{\titlecite}[2]{``#1''~\cite{#2}}

\title{{\small CS224U Project} \\ Discourse Disentanglement \\ \small{Final Report}}
\author{Julius Cheng \\
  {\tt juliusc@stanford.edu}
  \\\And
  Thomas Dimson  \\
  {\tt tdimson@cs.stanford.edu}
  \\\AND
  Milind Ganjoo \\
  {\tt mganjoo@stanford.edu}
}
\date{}
\begin{document}
\maketitle

\section{Introduction}
Our project focuses on conversation disentanglement, which broadly refers to
the problem of guessing the structure of a multi-party, asynchronous
conversation. In particular, we discuss the problem of \textit{conversation
reconstruction}, where machine-learning is employed to recover a lost
relationship between messages.

We concentrate on modern internet forums such as forums such as 
\textit{Hacker News}\footnote{\url{http://news.ycombinator.com}} 
and \textit{Reddit}\footnote{\url{http://www.reddit.com}}. 
On these forums, users is able to seed a conversation tree by posting a 
\textit{root message}, and participants reply and to begin sub-discussions 
underneath this root. The forums are characterised by a complex 
discussion tree that emerges as conversations drift and sub-discussions form.

In our conversation reconstruction task, we analyse a time-ordered list
of messages and construct a candidate conversation tree, presumed to be the structure
that was lost. On a high level, we minimize the reconstruction
error when forum conversation trees trees become lists.

Our research aims to answer three questions:
\begin{itemize}
  \item To what degree does semantic knowledge allow a machine to understand the 
    flow of online forum discussions?
  \item How much does non-semantic information, such as timestamps and poster karma,
    aid the system?
  \item Does performance vary across domains? Do factors such as tree depth,
    length of messages, and similarity of messages affect our ability to
    reconstruct conversations? 
\end{itemize}

We have made the code and datasets for our project available online
\footnote{\url{https://github.com/cosbynator/discussion-disentanglement-cs224u}} on the
GitHub web hosting service.

\section{Prior Work}
\label{sec:prior_work}
Past work in conversation reconstruction has been limited in scope 

As discussed in our literature review, there has been some limited prior 
work in reconstructing threaded discussion in online conversions. 

\subsection{Forum-style Disentanglement}
\label{sec:threaded}
We define \textit{forum-style disentanglement} as the task of creating 
graphical parent-child links between posts under a related topic. The primary characteristic 
of \textit{forum-style} problems are longer, well formed posts 
linked by relatively unsubjective ground truth links.

In \titlecite{Recovering implicit thread structure in newsgroup style
conversations}{Wang2008a}, the authors address reconstructing thread structure
using using message similarity alone. They take a flat time-ordered
conversation and use a similarity matrix to add graphical links between
messages. They claim that flat conversation structure limits further data
analysis, such as identification of significant contributions under a topic.

The basis of their similarity algorithm is TF-IDF: they
construct a lower triangular cosine-distance matrix of posts and use rules 
for conversion into a graph adjacency matrix. Their baseline is thresholding the
matrix, which they improve by limiting the column-distance (window size), and discounting
the matrix by the time lag between elements. The authors use a grid search to
determine right value for the threshold and discount parameters.

The dataset used by \texttt{LegSim} \cite{Wang2008a}, consists of 28
\textit{topics} and a total of 478 \textit{messages}. Each topic is a
discussion about a hypothetical legislative bill, and the messages are
reactions written by students in a political science class. The data itself is
flat, but the authors manually create directed links representing parent-child
relationships. It struck us as peculiar that the authors chose to work with
such a small corpus and rely on manually annotated ground truth instead of
using threaded ground truth.

Accordingly, the authors evaluate their predicted annotations using precision/recall of
their graph edges between messages. They identify their best results using
thresholding and a time-based discount for a seemingly-poor result of 0.40 
F1-score. It is unlikely that higher scores could be achieved without reaching
beyond text similarity. That said, the work may provide a good baseline system
for us to evaluate against. At the very least, we will consider a learning
system instead of a rule-based system and utilizing features besides post
similarity.  Furthermore, we need to use a dramatically larger corpus and without manually
annotated links.

Unlike \cite{Wang2008a}, the authors of in \titlecite{Learning online
discussion structures by conditional random fields}{Wang2011a} take a machine
learning approach using similar features. Their approach uses conditional
random fields and is able to utilize features for link prediction without
having to model their dependent distributions.

The \cite{Wang2011a} paper defines a conditional distribution over the reply
relationship given a post sequence and formulates it as a Maximum a Posteriori
(MAP) problem. A crucial component of this task is defining a set of features
that capture dependencies between posts. The paper describes two categories of
features. The first kind depend only on attributes drawn from a pair
of posts, such as content similarity score, recency, author name reference, and
indicator variables that give special importance to posts made last or first in
a given thread. The second kind also consider the parent assignments of
the posts, which allows scoring of characteristics like the
similarity between  post and the \emph{parent} of another post to determine
content propagation, or accord importance to posts that reply to the same author
and are also written by the same author (which is indicative of friends carrying
out a back-and-forth conversation in a thread). The weights for their full set of
13 features is trained through a supervised classifier that maximizes the
conditional likelihood for the reply sequence.

The conditional random field approach is novel for the number
of interesting features it introduces, and it also opens up the possibility of
extending the existing framework by adding more features.

\cite{Wang2011a} also introduces the novel \emph{path accuracy} metric, which
calculates the proportion of correct paths from each node to the root in a
conversation tree. This approach, however, would work best for a threaded
conversation that begins from a single topic or node, and seems difficult to extend
to IRC-style conversation where multiple topics might
be discussed in parallel. Still, we imagine it possible to usefully average the path
accuracy for paths to multiple such nodes.

Similar to \cite{Wang2011a}, in \titlecite{Reconstruction of threaded
conversations in online discussion forums}{Aumayr2011a}, the authors attempt machine learning to 
tackle the problem of reconstructing conversation trees in a traditional
vBulletin-powered online forum. Here, each forum consists of a \textit{board}, which
contains many \textit{threads}, each of which contains a series of
\textit{posts} by users. Unlike \cite{Wang2008a}, the motivation for their work
comes from reconstructing reply structure even after it has been lost in the
source database. This assumes that there was thread-structure in the original
data.

The authors report their best results using a C4.5 decision tree to classify
pairs of posts as in a (parent, reply) relationship or not. Their features are 
a mix of shallow textual and post metadata features: TF-IDF weighted text distance, 
presence of quotes and a user's name, post-distance and time difference. 
Since the classification is binary, it 
is unclear how the authors cope with ambiguity (e.g $(p1, r)$ and $(p2,r)$ both 
classified as reply pairs). Although post distance is their best feature, we are skeptical
that this will generalize to message boards that have voted or moderated posts.

The authors collected their data from two years of forum posts on
\url{boards.ie}. When browsing a \textit{thread}, users have a choice of either
pressing ``quote'' (i.e. replying to a post) or pressing ``reply'' (i.e.
replying to a thread). The choice is retained in the dataset and used in a
tree form as ground truth data. Without much justification, the authors eliminate
threads with too few or too many replies, and then split their data 50/50 into 
test and training to make 213,800 posts in total.

Their results are evaluated using precision and recall of individual
(post, reply) pairs. They report impressive results - an F1-score of 0.925 using their best
classifier. The results are contrasted to a baseline classifier that always
identifies the previous post as the parent of a reply, achieving 80\% F1-score. 
They also contrast their results with implementations from two other papers we looked at,
\cite{Elsner2008a} and \cite{Wang2008a}, achieving 88\% F1 and 44.4\% F1-score
respectively. Although their results are better than the conditional random field approach 
in \cite{Wang2011a}, the differences in datasets make a direct comparison difficult.
The forums of \url{boards.ie} lend themselves to linear discussion and it seems
unlikely that the authors last-post baseline will have as much success in 
other types of forums or chat.

\subsection{Chat Disentanglement}
\label{sec:chat}
We identify the \textit{chat disentanglement} problem as grouping
shorter conversational-style messages into coherent sub-conversations. Like 
Section \ref{sec:threaded}, authors create graphical links between messages. 
In contrast, the length of each message in a \textit{chat-style} is shorter, 
and ground truth data is quite subjective (what constitutes a sub-conversation?).

In \titlecite{You talking to me? a corpus and algorithm for conversation
disentanglement}{Elsner2008a}, the authors treat disentanglement as a clustering problem between 
utterances rather than speakers. They note that speaker-based clustering is
particularly problematic when conversation topics change.

Their dataset consists of manually annotated conversations in the \#Linux channel of 
the Freenode IRC network. Each annotation is a mapping to conversion identifiers
and contains information about schisms (change of 
topic) or minor digressions. It is worth investigating whether this 
manual annotation scheme can be applied to a different corpus, 
which is less topical and where the demarcations between conversations 
may be less clear. We anticipate the choice
of a more topical corpus would yield better classification accuracy.

\cite{Elsner2008a} first uses a MaxEnt classifier to decide whether a given pair of
utterances belongs to the same conversation. It then attempts to cluster the
utterances to maximize the weighted accuracy of the classifier. However, since
this is a correlation clustering problem (which is NP-complete), the paper uses
a greedy search approximation to clustering. This yields reasonable
results (71\% F1-score for the classifier against a 57\% majority-class baseline); 
however, we anticipate using the paper's approach as a starting point only.

The paper's model also uses more features that could transfer to
our own supervised learning model. These features include pause length
between utterances and word repetitions bucketed according to unigram
probability to indicate an ongoing conversation about something.

\titlecite{Context-based message expansion for disentanglement of interleaved 
text conversations}{Wang2009b} expands on the above study by distinguishing
\textit{context-free} and \textit{context-sensitive} message models, where the 
former refers to using data in the message alone, including word features and the
time-stamp. Most of the other papers we review do not explicitly make this 
distinction, as it seems that correlating messages with previous messages is 
natural idea. It appears that at this study was among the first of its 
kind to use context-sensitive data at the date of publishing.

This study augments a basic set of word features with author context, 
conversational context, and temporal context of a message. Author context 
refers to all other messages written by the author, and conversational context 
refers to all messages by participants in a sub-conversation, 
where the participants are determined by all other people referred to by name 
by the author of a message. Temporal context includes the entire message 
history. Each of these contexts are assigned a probability based on a normal 
distribution over time-distance. Each author and temporal context message is 
scaled by the distance between the time of the message of interest, and the 
context message. Conversational context is scaled by the time since the 
context message author was mentioned.

These contexts are used to ``expand'' a message by including all other 
conversational messages directly in the analysis, scaled by the time-distance 
probability. Thus, context expansion can be thought of as adding fractions of 
word frequency to a message.

Finally, thread detection is performed using clustering. The algorithm iterates 
through the messages, starting with one cluster containing the root message, 
using cosine similarity to determine whether a new message reaches a distance 
threshold to join a cluster, or whether it should start a new cluster. The 
result shows that context features outperform \cite{Elsner2008a} and
are within 88\% of the human annotation performance.

This paper provides insights into additional features that our project could 
use. The idea of expanding a message with probabilistic context is 
interesting, one that we will likely explore.

A very different approach is seen in \titlecite{Hierarchical conversation
structure prediction in multi-party chat}{Mayfield2012a}, the authors present
a model for chat disentanglement that annotates three types of information: 1) 
Negotiation labels, or whether an utterance is providing or receiving 
information, 2) sequences, or exchanges of information, and 3) threads, the 
outcome of the standard disentanglement problem.

The learning model is divided accordingly to predict these classes of
annotation. The three classifiers mainly use n-gram features, including simple
term frequency and TF-IDF. Not much discussion is given on the choice of these
features, and why each used different ones. For example, the Negotiation
classifier uses unigrams and bigrams frequencies and part-of-speech tags, while
the Previous Cluster classifier, which predicts sequences, uses TF-IDF. 

\cite{Mayfield2012a}'s algorithm performs two passes over the data, 
one in which negotiations and sequences are generated, followed by a pass
that groups sequences together to form threads. For sequence prediction, 
the model has five hard rules regarding the
structure of the Negotiation labels. For example, within the same sequence, the
provider of the information cannot be the same as the receiver. Also, in a
sequence, at most one continuous series of utterances may be made by the
information provider. These constraints are defined as an integer linear
programming problem and are used by the model to prevent classifications that
break domain rules. Interestingly, the human annotators of the data set are
expected to conform to these rules, guaranteeing that these rules are
“correct.”

The dataset used was weekly chat meetings for a cancer support group, with 45
conversations bucketed equally between 2 participants, 3-4, and at least 5. The
results were compared against baselines such as treating every message against
a new sequence. It also compared against results in negotiation labeling,
sequence labeling, and thread detection from a second set of human annotators,
performing better in some cases.

This study is very relevant to our project and introduces two major ideas to
consider. One is the idea of using knowledge exchange as a feature of
consideration in disentangling threads. The second is the two-pass model of
first constructing sequences and merging them into threads in the second pass.
In modeling what a thread is, it can be useful to define its components,
find them, and merge them to produce threads. However, we find that the
data collection methodology requires significant expertise, given the task of
understanding deep linguistic concepts while adhering to logic-based domain
constraints.

\subsection{Speech/Dialogue Acts}
Some authors consider the problem of speech disentanglement as traversing 
a finite state automaton between different \textit{speech acts}, such as
``question'', ``answer'' or ``statement''. Although this is a classic problem in
natural language processing, many authors evaluate their success extrinsically 
using discourse disentanglement.

The authors of \titlecite{Mixed Membership Markov Models for Unsupervised
Conversation Modeling}{Paula} build an unsupervised Hidden Markov Model that 
maps speech acts to hidden states. They believe that the words of a message 
in a conversation are generated according to language models associated
with a state or speech act. 

While the use of Hidden Markov Models is not new, the model presented in this
paper allows a message to be in a \emph{mixture} of states rather than exactly
one class, as in most HMMs. In this sense it is similar to topic models such as
LDA that assume a distribution over multiple classes (topics).

The most exciting contribution of this paper is the discovery of hidden states
that actually correspond to speech acts, for example, a collection of
question-related pronouns and words corresponding to a speech act beginning a
forum post about a technical question. The revelation of the hidden structure of
a conversation motivates further investigation into clustering messages by the
speech acts they convey, as investigated in the next paper on Twitter
unsupervised modeling.

Like \cite{Paula}, the authors of \titlecite{Unsupervised modeling of Twitter
conversations}{Ritter2010a} consider unsupervised tagging of speech acts.
The paper describes an unsupervised LDA-like approach to act tagging 
and utilizes Twitter \textit{conversations} for
training purposes. The model was evaluated extrinsically by reconstructing 
the original order of a scrambled conversation. The authors describe their motivation as
generalizing act tagging to remove the human required in supervised approaches. 

The authors constructed a dataset of 1.3 million Twitter \textit{conversations}, 
where conversations are alternating series of reply-tweets between users. After filtering out shorter 
and longer conversations, the dataset was used for unsupervised training and evaluation.

The paper tells a generative story of Twitter conversations where a sequence of dialogue
acts are chosen for each post, with words filled in by an act-specific language model.
Because this is biased to model \textit{topics}
instead of \textit{acts}, the model is refined to include a topic 
for the entire conversation. The language model is also refined to choose words from
the conversation topic, the dialog act of the tweet, and from general English. The result
is a variant of LDA and much of the paper is dedicated to filling out the details of the model.

Most relevant to us are the two forms of evaluation they propose. For
qualitative evaluation they create a probabilistic finite automaton with
transitions between speech acts through a conversation. The speech acts
themselves are hand-labeled using the top-40 words in each act cluster. The 
authors admit this is a very subjective and domain-specific evaluation. As such,
they propose a quantitative evaluation method that quantifies the ability of
the model to predict the ordering of posts in a conversation. They took
1,000 randomly sampled conversations, computed all permutations
of each conversation and then chose the ordering that had the maximum
probability according to the model. They compared this ordering to the gold
ordering using Kendell $\tau$ distance. For a baseline they chose a bigram
model, the details of which are omitted from the paper. They report a ten-fold improvement
over the baseline, or about a 0.3 $\tau$-correlation. It is encouraging to know
that conversations appear to have a \textit{role} structure. We believe we can get a lot of mileage
by adding in additional features like \cite{Wang2008a}'s post-similarity.

Unlike other speech act papers, \titlecite{Classifying Dialogue Acts in Multi-party Live Chats}{Kim2012} 
focuses on classifying utterances
in multi-party chat scenarios. The basic unit
of analysis is a contiguous message written by an individual. The
experiment classifies messages into discourse acts such as a greeting, yes/no
question, request for information, or 11 other categories defined in previous
works, with two new ones introduced here.

The authors define several classes of features, then runs various classifiers on
combinations of them. The features include TF-IDF and boolean unigrams, with
and without stemming and lemmatization. They also use special keywords 
that indicate various discourse acts. Other
features are 1) structural attributes, such as length of message, location of
message within the dialogue, and user ID of the speaker, and 2) utterance
interactions, like the classification and content of previous messages, and
previous utterances by the author.

Their experiment uses Naive Bayes, SVM, and Conditional Random Fields on two
datasets, one topic-specific forum and one casual live chat. The best
performance in both datasets approaches 100\% on a model using bag-of-words,
keywords, and previous utterances, but not structural information. The error
analysis shows that most errors pertain to ambiguity between pairs of dialogue
acts, like statements versus responses, and over certain words that appear
frequently in multiple dialogue acts. Finally, the authors conclude that the
failure of structural information compared to previous works is likely due to
entanglement.

While this work is not directly about entanglement, it produces good results in
two entangled datasets. The authors conclude that further work on
disentanglement is needed to improve their results, but we suggest that their
results show promise in the problem. Using
dialogue acts without structural information may be a useful feature
for us. This study shows that techniques applied to forums can work well
in live chat scenarios, and vice versa, which indicates that our future model
could potentially succeed in both settings.

\section{Conclusion}

We have surveyed nine studies on dialogue in multi-party entangled online 
chat or forum settings. We reviewed several experiments that directly address
the problem of disentangling conversation threads in long-form forum/newsgroup messages,
short-form chat or Twitter conversations. Most approaches
invariably involve deconstructing messages into feature vectors and running a 
well-known machine learning algorithm such as HMM, CRF, and SVM over them.
There is some variety in how threads are constructed: \cite{Aumayr2011a} uses
a binary classification on whether two messages have a parent-child
relationship, while \cite{Mayfield2012a} cluster messages into threads with 
a cosine similarity metric. Our initial approach to the project will be to 
start with a model that emulates some of the above.

Beyond disentanglement, we reviewed three papers that discuss annotating 
speech acts. \cite{Ritter2010a} and \cite{Paula} use unsupervised clustering 
to discover speech acts, while \cite{Kim2012} use supervised learning on a set 
of pre-fabricated labels based on linguistic theory. Based on the findings of 
\cite{Kim2012}, we expect speech acts to be beneficial features, and we hope 
to discover a novel way to manually create or learn rules for sequencing 
utterances of various combinations of speech acts. Based on the findings of
\cite{Ritter2010a} and \cite{Paula}, we realize that it is possible to induce 
a set of speech acts during learning as to avoid using artificial categories 
and requiring additional human annotation.

We have found each of these papers to be relevant, useful, and inspirational
for our project on multi-party dialogue disentanglement.


\subsection{Dataset}
\label{section:dataset}
Our approach differs from most of those listed in Section \ref{sec:approaches}
by the scale of our approach: we are trying to reconstruct complex
conversations with large branching numbers.  In that vein, we extract data
from Reddit, an online link-sharing and discussion site. To ensure all
conversations have a root element, we used the popular AskReddit subforum
where users initiate topics of conversation with a question. We treat the
initial post as the root of a conversation tree, and each top-level comment as
a child of the root. Each message node is annotated with various data such as
timestamp, username, and more.

To grab the data, we wrote a quick JRuby crawler and took a small sample (342)
of the top AskReddit posts over the last month. The crawler was written to
make data gathering easy and we anticipate creating a bigger dataset to report
our final results. Table \ref{table:stats} shows some aggregated statistics of
the preliminary crawl. In particular, we have an average of 200 messages per
tree and a branching factor within a rounding error of 1.0. The root node has
an average branching factor of 55.50, suggesting that discussions are wide
while sub-discussions tend to be narrow. It is also important to note that
standard deviation for many of our statistics are often quite large relative
to the value, suggesting the wide variance in conversations on Reddit.

\begin{table}[h]\footnotesize
 \begin{tabular}{| l | l | l |} 
   \hline
   \textbf{Statistic} & \textbf{$n / \mu$} & \textbf{$\sigma$} \\
   \hline
    \# Conversation Trees &  342 & \\
    \# Conversation Nodes (Posts) & 68639 & \\
    \# Nodes Per Tree &  200.70 & 200.20 \\
    Mean Branching Factor & 1.00 & 4.20 \\
    Mean Root Branching Factor &  55.50 & 57.91 \\
    Mean Node Depth  & 3.56 & 3.40 \\
    Mean Parent Lag & 7767.96s & 11331.07s \\
    Mean Body Length & 200.43 c & 483.07 \\
   \hline
  \end{tabular}
  \caption{Data statistics of our preliminary conversation dataset}
  \label{table:stats}
\end{table}

We have divided the input data randomly into training / development / test 
sets at a 70\% / 10\% / 20\% split.

\section{Our Model}
\subsection{Approach}
\label{sec:approach}
We are approaching conversation reconstruction as a \textit{classification} problem
between parent and children messages. We take a threaded conversations from a modern internet forum 
(described in Section~\ref{section:dataset}), flatten them into a time-sorted list, and them feed
them into our reconstruction \textit{classifier} that attempts to recreate the tree
with only message-level information. The details our reconstruction algorithm are 
described in Section \ref{sec:classifier}.

\subsection{Evaluation}
For now, we are primarily considering F1 score as our evaluation metric. We
compute precision and recall based on how many of our guessed 
$(parent,child)$ conversation edges match the gold tree. We are considering 
two versions of F1: one were precision and recall is calculated pairwise 
across all messages in all conversations, and another where F1 is calculated 
per-conversation, and the scores are averaged in the end. We expect to have 
very poor performance  in large conversations, so we expect the average F1
score metric to be more forgiving. Both metrics have been implemented in our 
code. Since our crawled dataset currently contains conversations of similar 
size, our distinct evaluation metrics result in scores that are equal to the 
4th decimal. We anticipate our final dataset will have more diversity.

Noting that it can be more important to get lower-depth subtrees correct, we
are also considering weighting a clustering metric such as $B^3$. Here, we 
can treat subtrees at a given depth as clusters, comparing against gold 
clusters at that depth. We could weight proportional to the inverse depth to 
reward us for capturing more of the general discourse with less sensitivity 
to individual placement.

\section{Results}
\label{sec:classifier}
However, our excitement at crossing the trivial baseline should be limited,
since we don't currently have a visualization of the results, and it's
possible that the SVM is still making random guesses, leading to 32\% F1 by
simply associating all messages with the root, for example. This is something
we still need to address by carefully visualizing the actual predicted trees.

\begin{table}[h]\footnotesize
 \begin{tabular}{| l | l | l |} 
   \hline
   \textbf{Statistic} & Pairwise F1 & Average-tree F1 \\
   \hline
    SVM baseline & 0.329 & 0.331 \\
    All-reply-to-root baseline & 0.278 & 0.278 \\
    One-thread baseline & 0.021 & 0.021 \\
   \hline
  \end{tabular}
  \caption{Data statistics of our preliminary conversation dataset}
  \label{table:results}
\end{table}

\section{Error Analysis}
%TODO
We should put something here.

\section{Conclusion and Next Steps}
%TODO
We should also put something here

\bibliography{final_report}{} 
\bibliographystyle{acl2012}

\end{document}
