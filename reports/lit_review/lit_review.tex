\documentclass{article}
\usepackage{url}

\title{CS224U Final Project \\Literature Review}
\author{Julius Cheng\\ Thomas Dimson \\ Milind Ganjoo}
\begin{document}
\maketitle

\section{Introduction}
Our project is focused on reconstructing threaded conversation trees
from a flattened representation. Intuitively, one could think of 
taking a post's comment tree (e.g. Reddit), flattening it, sorting by time, performing
hierarchical clustering using linguistic phenomena and measuring performance 
against the original comment trees. As we have
discovered, this project treads into the territory of \textit{discourse disentanglement}.
While our particular problem does not appear to have been tackled in the past,
there is a good amount of literature about disentangling flows of conversation
in traditional internet forums, IRC chats, Twitter and beyond.

\section{Literature Overview}
\subsection{Recovering implicit thread structure in newsgroup style
conversations}
In \cite{Wang2008a}, the authors tackle the problem of reconstructing thread
structure using message similarity. They take a flat
time-ordered conversation and use a similarity matrix to add graphical
links between messages. They claim that flat conversation structure
limits further data analysis, such as identification of significant
contributions. 

The paper calls the task \textit{thread recovery},
adding graphical links to message collections with a known root.
The basis of their algorithm is message similarity through TF-IDF: they
construct a lower triangular cosine-distance matrix of posts and use rules 
for conversion into a graph adjacency matrix. Their baseline is thresholding the
matrix, which they augment by limiting the column-distance (window size), and discounting
the matrix by the time lag between elements. The authors use a grid search to
determine right value for the threshold and discount parameters.

\cite{Wang2008a}'s dataset, \texttt{LegSim}, is consists of 28 topics with over 
478 messages . Each topic is a discussion about a hypothetical legislative bill, 
and the messages are written by students discussing the bill in a political science class. 
The data itself is flat, but  the authors manually create directed links representing
parent-child relationships. 

Their evaluation process was standard precision/recall based on the predicted
and ground truth graph edges between messages. The authors' report the best
results with thresholding and a time-based discount with a seemingly-poor result of 0.40 
F1-score. The baseline system achieves an F1-score of 0.28 on their
dataset. It is unlikely that higher scores could be achieved without reaching
beyond text similarity. That said, the work may provide a good baseline system for us to evaluate against. 
At the very least, we will consider a learning system instead of a rule-based system 
and utilizing features besides post similarity.
If nothing else, we need to use a dramatically larger corpus.

\subsection{Reconstruction of threaded conversations in online discussion forums}
\cite{Aumayr2011a}'s approach most closely resembles our own. The
authors consider the problem of reconstructing conversation trees in
a traditional vBulletin-powered online forum. The forum consists of a
\textit{board}, which contains many \textit{threads},
each of which contains a series of \textit{posts} by users. Unlike \cite{Wang2008a}, the motivation for
their work comes from reconstructing reply structure even after it has been lost
in the source database. This assumes that there was thread-structure in the original
data.

The authors use a learned classifier to classify pairs of posts as (parent,
reply) or other. Their features are a mix of shallow textual and post
metadata features: TF-IDF weighted text distance, presence of quotes and a
user's name, post-distance and time difference. In the end, they report their 
best results using a C4.5 decision tree. Since the classification is binary, it 
is unclear how the authors cope with ambiguity (e.g $(p1, r)$ and $(p2,r)$ both 
classified as reply pairs). Although post distance is their best feature, we are skeptical
that this will generalize to message boards that have a different ordering.

The authors collected data from two years of forum posts on
\url{boards.ie}. When browsing a \textit{thread}, users have a choice of either
pressing ``quote'' (i.e. replying to a post) or pressing ``reply'' (i.e.
replying to a thread). The choice is retained in the dataset and used in a
tree form as ground truth data. Without much justification, the authors eliminate
threads with too few or too many replies, and then split their data 50/50 into 
test and training to make 213,800 posts in total.

Their results are evaluated using precision and recall of individual
(post, reply) pairs. They report impressive results - an F1-score of 0.925 using their best
classifier. The results are contrasted to a baseline classifier that always
identifies the previous post as the parent of a reply, achieving 80\% F1-score. 
They also contrast their results with implementations from two other papers we looked at,
\cite{Elsner2008a} and \cite{Wang2008a}, achieving 88\% F1 and 44.4\% F1-score
respectively.

\subsection{Unsupervised modeling of Twitter conversations}
\cite{Ritter2010a} considers conversation from the perspective of 
\textit{dialogue acts}, such as ``question'', ``answer'' or ``statement''. Unlike our other papers,
\cite{Ritter2010a}'s focus is on acts and not disentanglement. The paper describes
an unsupervised LDA-like approach to act tagging and utilizes twitter \textit{conversations} for
training purposes. The model was evaluated extrinsically by reconstructing 
the original order of a scrambled conversation. The author's describe their motivation as
generalizing act tagging to remove the human required in supervised approaches. 

The authors constructed a dataset of 1.3 million twitter \textit{conversations}, 
where conversations are alternating series of reply-tweets between users. After filtering out shorter 
and longer conversations, the dataset was used for unsupervised training and evaluation.

The paper tells a generative story of twitter conversations where a sequence of dialogue
acts are chosen for each post, with words filled in by an act-specific language model.
Because is biased to model \textit{topics}
instead of \textit{acts}, the model is refined to include a topic 
for the entire conversation. The language model is also refined to choose words from
the conversation topic, the dialog act of the tweet, and from general English. The result
is a variant of LDA and much of the paper is dedicated to filling out the details of the model.

Most relevant to us are the two forms of evaluation they propose. For
qualitative evaluation they create a probabilistic finite automaton with
transitions between speech acts through a conversation. The speech acts
themselves are hand-labelled using the top-40 words in each act cluster. The 
authors admit this is a very subjective and domain-specific evaluation. As such,
they propose a quantitative evaluation method that quantifies the ability of
the model to predict the ordering of posts in a conversation. They took
1,000 randomly sampled conversations, computed all permutations
of each conversation and then chose the ordering that had the maximum
probability according to the model. They compared this ordering to the gold
ordering using Kendell $\tau$ distance. For a baseline they chose a bigram
model, the details of which are omitted from the paper. They report a ten-fold improvement
over the baseline, or about a 0.3 $\tau$-correlation. It is encouraging to know
that conversations appear to have a \textit{role} structure. We believe we can get a lot of mileage
by adding in additional features like \cite{Wang2008a}'s post-similarity. 


\section{Conclusion}

\bibliography{lit_review}{}
\bibliographystyle{alpha}

\end{document}
