\documentclass[10pt]{article}
\usepackage{acl2012}
%\usepackage[margin=0.75in]{geometry} if we are desperate for more space
\usepackage{times}
\usepackage{tabularx}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}

\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.0cm}    % Expanding the titlebox

\newcommand{\titlecite}[2]{``#1''~\cite{#2}}

\title{{\small CS224U Project} \\ Discourse Disentanglement \\ \small{Final Report}}
\author{Julius Cheng \\
  {\tt juliusc@stanford.edu}
  \\\And
  Thomas Dimson  \\
  {\tt tdimson@cs.stanford.edu}
  \\\AND
  Milind Ganjoo \\
  {\tt mganjoo@stanford.edu}
}
\date{}
\begin{document}
\maketitle

\section{Introduction}
Our project focuses on conversation disentanglement, which broadly refers to
the problem of guessing the structure of a multi-party, asynchronous
conversation. In particular, we discuss the problem of \textit{conversation
reconstruction}, where machine-learning is employed to recover a lost
relationship between messages.

We concentrate on modern internet forums such as 
\textit{Hacker News}\footnote{\url{http://news.ycombinator.com}} 
and \textit{Reddit}\footnote{\url{http://www.reddit.com}}. 
On these forums, users are able to seed a conversation tree by posting a 
\textit{root message}, and participants reply and to begin sub-discussions 
underneath this root. The forums are characterised by a complex 
discussion tree that emerges as conversations drift and sub-discussions form.

In our conversation reconstruction task, we analyse a time-ordered list
of messages and construct a candidate conversation tree, presumed to be the structure
that was lost. On a high level, we minimize the reconstruction
error when forum conversation trees flatten into lists.

Our research aims to answer three questions:
\begin{itemize}
  \item To what degree does semantic knowledge allow a machine to understand the 
    flow of online forum discussions?
  \item How much does non-semantic information, such as timestamps and poster karma,
    aid the system?
  \item Does performance vary across domains? Do factors such as tree depth,
    length of messages, and similarity of messages affect our ability to
    reconstruct conversations? 
\end{itemize}

As we show, answering these questions correlates to the performance of a system
in the conversation reconstruction task. 

We have made the code and datasets for our project available online
\footnote{\url{https://github.com/cosbynator/discussion-disentanglement-cs224u}} on the
GitHub web hosting service.

\section{Prior Work}
\label{sec:prior_work}
While there has been prior work in conversation reconstruction, none examine
the exact sub-domain we did. There is a general divide between authors
who examine traditional online forums, where message are long but exhibit a linear
reply structure, and authors who examine reconstruction in the context of internet chat, 
where messages are relatively short and directed. Approaches taken by authors also vary considerably:
some employ rule-based systems, while others choose heavily probabilistic models such as conditional
random fields. In certain cases, conversation reconstruction has been an explored as an evaluation metric 
for unsupervised models that discover \textit{speech acts} in conversational data.

We identified \textit{forum-style disentanglement} as the task of creating 
graphical parent-child links between posts under a related topic. The primary characteristic 
of \textit{forum-style} problems are longer, well formed posts 
linked by relatively unsubjective ground truth links. In an early
attempt at this problem domain, \cite{Wang2008a} address reconstructing
thread structure using message similarity alone. They employ a rule-based
system that creates graphical links between messages to represent a connection.
They employ a small (478) message corpus taken from a political science domain. 
Peculiarly, the authors manually created their own ground truth links and ended
up without an edge-wise F1 score of 0.40.

Unlike the rule-based system of \cite{Wang2008a}, the authors of 
in \cite{Wang2011a} employ machine learning to aid in their task. They employ
the machinery of conditional random fields for link prediction.
%This seems like a candidate to put something else into
The the authors defines a conditional distribution over the reply
relationship given a post sequence and formulates it as a Maximum a Posteriori
(MAP) problem. A crucial component of this task is defining a set of features
that capture dependencies between posts. The paper describes two categories of
features. The first kind depend only on attributes drawn from a pair
of posts, such as content similarity score, recency, author name reference, and
indicator variables that give special importance to posts made last or first in
a given thread. The second kind also consider the parent assignments of
the posts, which allows scoring of characteristics like the
similarity between  post and the \emph{parent} of another post to determine
content propagation, or accord importance to posts that reply to the same author
and are also written by the same author (which is indicative of friends carrying
out a back-and-forth conversation in a thread). The weights for their full set of
13 features is trained through a supervised classifier that maximizes the
conditional likelihood for the reply sequence.

Similarly, in \cite{Aumayr2011a}, the authors attempt machine learning to 
tackle the problem of reconstructing conversation trees in a traditional
vBulletin-powered online forum. This assumes that there was thread-structure 
in the original data. The authors report their best results using C4.5-style
decision trees based on classifying whether pairs of posts are in a $(parent, child)$
relationship or not. Their features are a mix of shallow textual and post 
metadata features: TF-IDF weighted text distance, 
presence of quotes and a user's name, post-distance and time difference. 
Although post distance is their best feature, is particularly suited to the 
near-linear structure of their domain.

We identify the \textit{chat disentanglement} problem as grouping
shorter conversational-style messages into coherent sub-conversations.
In contrast to forum-style disentanglement, the length of each messages tend to be shorter
and ground truth data is quite subjective (what constitutes a sub-conversation?).

In early work, \ref{Elsner2008a} treat disentanglement as a clustering problem between 
utterances. Their dataset consists of manually annotated IRC conversations that contain
information about schisms in topics and minor digressions. Similar to most authors, 
they employ a machine-learning model using a MaxEnt classifier to decide whether a given pair of
utterances belongs to the same conversation. It then attempts to cluster the
utterances to maximize the weighted accuracy of the classifier. As with much other
work, they employ pause length and message similarity as key features in
their classifier.

\ref{Wang2009b} expands on the above study by distinguishing
\textit{context-free} and \textit{context-sensitive} message models, where the 
former refers to using data in the message alone, including word features and the
time-stamp.  This study augments a basic set of word features with author context, 
conversational context, and temporal context of a message. Author context 
refers to all other messages written by the author, and conversational context 
refers to all messages by participants in a sub-conversation, 
where the participants are determined by all other people referred to by name 
by the author of a message.  Afterward computing their features, they perform
clustering to identify different threads of conversation.

A very different approach is seen in \cite{Mayfield2012a}, the authors present
a model for chat disentanglement that annotates three types of information: 1) 
Negotiation labels, or whether an utterance is providing or receiving 
information, 2) sequences, or exchanges of information, and 3) threads, the 
outcome of the standard disentanglement problem. Their algorithm performs 
two passes over the data,  one in which negotiations and sequences are generated, 
followed by a pass that groups sequences together to form threads. Uniquely,
their data was hand-annotated from verbal chat meetings for a cancer support group.
They report results that occasionally beat human annotators.

Finally, some authors consider the problem of speech disentanglement as traversing 
a finite state automaton between different \textit{speech acts}, such as
``question'', ``answer'' or ``statement''. Although this is a classic problem in
natural language processing, it is natural evaluate their success extrinsically 
using discourse disentanglement. The authors of \cite{Paula} build an 
unsupervised Hidden Markov Model that  maps speech acts to hidden states. 
They believe that the words of a message 
in a conversation are generated according to language models associated
with a state or speech act. They report hidden states
that correctly correspond to speech acts, for example, a collection of
question-related pronouns and words corresponding to a speech act beginning a
forum post about a technical question. As we show in Section \ref{sec:dataset}, 
our results with their model were not as successful.

The authors of \cite{Ritter2010a} consider unsupervised tagging of speech acts.
The paper describes an unsupervised LDA-like approach to act tagging 
and utilizes Twitter \textit{conversations} for
training purposes. Fittingly, their model was evaluated 
extrinsically by reconstructing the original order of a scrambled
conversations in Twitter. Their model corresponds to a generative story
where words are filled by either the conversation topic, the dialog
act of the tweet or from general English.

Unlike other speech act papers, \cite{Kim2012} 
focuses on classifying utterances
in multi-party chat scenarios. The basic unit
of analysis is a contiguous message written by an individual. The
experiment classifies messages into discourse acts such as a greeting, yes/no
question, request for information, or 11 other categories defined in previous
works, with two new ones introduced here.

The authors define several classes of features, then runs various classifiers on
combinations of them. The features include TF-IDF and boolean unigrams, with
and without stemming and lemmatization.  The best
performance in both datasets approaches 100\% on a model using bag-of-words,
keywords, and previous utterances, but not structural information. 
The authors conclude that the failure of structural information 
compared to previous works is likely due to entanglement.

\subsection{Dataset}
\label{section:dataset}
Our approach differs from most of those listed in Section \ref{sec:prior_work}
by scale: we are trying to reconstruct large, complex
conversations with a unpredictable branching structures. After reviewing existing datasets,
we decided that gathering our own was the best option. We looked at two
online forums: Hacker News and Reddit. To ensure that all conversations had a long
and well formed root node, we chose discussions that revolved around question and answer 
(AskHN and AskReddit respectively). 

For our Reddit dataset, we wrote a small JRuby script that utilized their API. Here,
we fetched a few pages of the listing for the ``top'' AskReddit discussions of the last
week, and downloaded the comments for each page in turn. Hacker News does not have
an official API, but the popular third party ``HNSearch'' API functions similarly.
In this case, we ran a query for all discussions with between 20 and 100 comments
that has the words ``Ask HN'' in the title. After rounding up the results (using
a timestamp-based hack to get around paging limits), we fetched the contents of
each discussion directly from its search identifier. Finally, we gathered all
the unique authors in the discussions and ran a disjunctive query to search for
their usernames. In both cases, we amalgamated the data, ran it through
CoreNLP's \texttt{tokenize, ssplit, pos, lemma} and \texttt{ner} annotators and
then serialized it on-disk for structured access.

\begin{table}[h]\footnotesize
  \begin{tabularx}{0.5\textwidth}{| l X |}
   \hline
   \textbf{Hacker News} & \\
   \hline
   Programming  & code, ?, language, web, python, data, good, time, server, work, php, java, app, programming, system, things, languages, windows, lot, make \\

  Sites  & ?, people, site, !, email, google, page, make, good, users, idea, app, design, find, great, search, time, user, sites, post \\

  Business  & ?, work, company, people, money, business, time, good, job, startup, make, pay, companies, working, lot, years, market, idea, year, find \\

  College,  Learning  & people, time, ?, good, work, things, make, lot, find, learn, school, life, years, read, !, thing, problem, programming, college, day \\
   \hline
   \textbf{Reddit} & \\
   \hline
  Americanism & people, work, ?, money, time, english, make, lot, world, job, free, language, good, things, country, american, war, reddit, white, edit \\

  Minutia & back, time, ?, guy, !, day, car, house, room, night, home, told, friend, put, shit, door, dad, people, work, started \\

  Family/Friends  & people, ?, time, life, years, school, good, things, love, make, day, feel, friends, thing, !, person, year, lot, family, girl \\

  Trolls/Memes  & !, ?, http:\/\/www.youtube.com\/watch?v, edit, movie, fuck, man, good, song, deleted, love, shit, fucking, favorite, reddit, link, make, god, video, great \\
   \hline
  \end{tabularx}
  \caption{Top words over four topics in our datasets}
  \label{table:lda}
\end{table}

After analysing both sets of data, we decided to focus our efforts on Hacker News.
While the AskReddit discussions tended to have a higher branching factor 
and more sub-discussions, they also had less of a topical focus, fewer words per message
and a higher presence of trolls. Table~\ref{table:lda} shows the top words associated
with topics on both Reddit and Hacker News gathered by performing latent dirichlet allocation (LDA)
on the data. Topics of Hacker News, such as business and programming, are 
what we would expect given that the site is organized around the startup community.
In contrast, the Reddit data more closely match what we would expect in an IRC
chatroom: broad topics that resemble watercooler-type conversations with trolls
and inappropriate jokes. As Section\ref{sec:results} shows, we are more successful
at finding attachment points when topics are more specialized. We were hoping that
some of the topics would be correlated with \textit{speech acts} in the data but 
none of them appear to be. As an experiment, we tried running the unsupervised speech act 
model of~\cite{Paula} but found the acts fell more along the lines of 
topics than speech acts. More investigation is needed to determine if this
is an inherent property of our data.


\begin{table}[h]\footnotesize
 \begin{tabular}{| l | l | l |} 
   \hline
   \textbf{Statistic} & \textbf{$n / \mu$} & \textbf{$\sigma$} \\
   \hline
   \# of Conversations Trees & 2476 & \\
   \# of Messages & 97414 & \\
   \# of Authors & 27861 & \\
   Messages per Tree & 39.34 & 43.09 \\
   Messaged Attached to Root &  18.20 & 20.24 \\
   Branching Factor & 0.97 & 3.29 \\
   Mean Depth & 2.96 & 2.77 \\
   Mean Message Tokens & 86.32 & 134.92 \\
   \hline
  \end{tabular}
  \caption{Data statistics of our Hacker News AskHN dataset}
  \label{table:stats}
\end{table}

Table \ref{table:stats} shows aggregate information on our Hacker News
dataset. The data consists of nearly a hundred thousand messages distributed
over 2476 different conversations, indicating that conversation trees 
are relatively full. On average, slightly less than half the messages
in a discussion are directly attached to the root element. This accounts
for the great performance of our baselines in \ref{sec:results}. 
The branching factor for any particular node is close to 1.0, indicating
that conversations tend to be wide at the top and then narrow down with
depth. The messages we examine have approximately 86 tokens per message,
which suggests that there may be enough semantic information to make
decisions. Finally, all statistics in the Hacker News dataset have a
a high standard deviation. We believe this reflects the chaotic nature of
online discussion - \textit{anyone} can participate and has their own
style of replying.

In all cases we divided the input data randomly into training / development
and test sets at a 70\% / 10\% / 20\% split. Except for our final results, we
conduct and report all of our experiments on our development sets.

\section{Our Model}
\subsection{Approach}
\label{sec:approach}
We are approaching conversation reconstruction as a \textit{classification} problem
between parent and children messages. We take a threaded conversations from a modern internet forum 
(described in Section~\ref{section:dataset}), flatten them into a time-sorted list, and them feed
them into our reconstruction \textit{classifier} that attempts to recreate the tree
with only message-level information. The details our reconstruction algorithm are 
described in Section \ref{sec:classifier}.

\subsection{Evaluation}
For now, we are primarily considering F1 score as our evaluation metric. We
compute precision and recall based on how many of our guessed 
$(parent,child)$ conversation edges match the gold tree. We are considering 
two versions of F1: one were precision and recall is calculated pairwise 
across all messages in all conversations, and another where F1 is calculated 
per-conversation, and the scores are averaged in the end. We expect to have 
very poor performance  in large conversations, so we expect the average F1
score metric to be more forgiving. Both metrics have been implemented in our 
code. Since our crawled dataset currently contains conversations of similar 
size, our distinct evaluation metrics result in scores that are equal to the 
4th decimal. We anticipate our final dataset will have more diversity.

Milind, Julius - can we put some more stuff here. 
Specifically, about the $B^3$ metric.

\subsection{Features}
\begin{table}[h]\footnotesize
 \begin{tabular}{| l | l | l | l | l | l | l |} 
   \hline
   \textbf{Statistic} & Pairwise F1 & Average-tree F1 & $B^3$ depth 1 & $B^3$ depth 2 & $B^3$ depth 3 & $B^3$ depth 4\\
   \hline
   All-reply-to-root Baseline & 0.476 & 0.488 & 0.641 & & & \\
        Linear-reply Baseline & 0.093 & 0.111 & 0.219 & 0.178 & 0.144 & 0.125  \\
   \hline
  \end{tabular}
  \caption{Development Set }
  \label{table:results}
\end{table}


\section{Results}
\label{sec:results}
However, our excitement at crossing the trivial baseline should be limited,
since we don't currently have a visualization of the results, and it's
possible that the SVM is still making random guesses, leading to 32\% F1 by
simply associating all messages with the root, for example. This is something
we still need to address by carefully visualizing the actual predicted trees.

\begin{table}[h]\footnotesize
 \begin{tabular}{| l | l | l |} 
   \hline
   \textbf{Statistic} & Pairwise F1 & Average-tree F1 \\
   \hline
    SVM baseline & 0.329 & 0.331 \\
    All-reply-to-root baseline & 0.278 & 0.278 \\
    One-thread baseline & 0.021 & 0.021 \\
   \hline
  \end{tabular}
  \caption{Data statistics of our preliminary conversation dataset}
  \label{table:results}
\end{table}

\section{Error Analysis}
%TODO
We should put something here.

\section{Conclusion and Next Steps}
%TODO
We should also put something here


\bibliography{final_report}{} 
\bibliographystyle{acl2012}

\end{document}
