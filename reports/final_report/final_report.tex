\documentclass[10pt]{article}
\usepackage{acl2012}
%\usepackage[margin=0.75in]{geometry} if we are desperate for more space
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{url}

\DeclareMathOperator*{\argmax}{arg\,max}
\setlength\titlebox{6.0cm}    % Expanding the titlebox

\newcommand{\titlecite}[2]{``#1''~\cite{#2}}

\title{{\small CS224U Project} \\ Discourse Disentanglement \\ \small{Final Report}}
\author{Julius Cheng \\
  {\tt juliusc@stanford.edu}
  \\\And
  Thomas Dimson  \\
  {\tt tdimson@cs.stanford.edu}
  \\\AND
  Milind Ganjoo \\
  {\tt mganjoo@stanford.edu}
}
\date{}
\begin{document}
\maketitle

\section{Introduction}
Our project focuses on conversation disentanglement, which broadly refers to
the problem of guessing the structure of a multi-party, asynchronous
conversation. In particular, we discuss the problem of \textit{conversation
reconstruction}, where machine-learning is employed to recover a lost
relationship between messages.

We concentrate on modern internet forums such as forums such as 
\textit{Hacker News}\footnote{\url{http://news.ycombinator.com}} 
and \textit{Reddit}\footnote{\url{http://www.reddit.com}}. 
On these forums, users is able to seed a conversation tree by posting a 
\textit{root message}, and participants reply and to begin sub-discussions 
underneath this root. The forums are characterised by a complex 
discussion tree that emerges as conversations drift and sub-discussions form.

In our conversation reconstruction task, we analyse a time-ordered list
of messages and construct a candidate conversation tree, presumed to be the structure
that was lost. On a high level, we minimize the reconstruction
error when forum conversation trees trees become lists.

Our research aims to answer three questions:
\begin{itemize}
  \item To what degree does semantic knowledge allow a machine to understand the 
    flow of online forum discussions?
  \item How much does non-semantic information, such as timestamps and poster karma,
    aid the system?
  \item Does performance vary across domains? Do factors such as tree depth,
    length of messages, and similarity of messages affect our ability to
    reconstruct conversations? 
\end{itemize}

We have made the code and datasets for our project available online
\footnote{\url{https://github.com/cosbynator/discussion-disentanglement-cs224u}} on the
GitHub web hosting service.

\section{Prior Work}
\label{sec:prior_work}
While there has been prior work in conversation reconstruction, none examine
the exact sub-domain we did. There is a general divide between authors
who examine traditional online forums, where message are long but exhibit a linear
reply structure, and authors who examine reconstruction in the context of internet chat, 
where messages are relatively short and directed. Approaches taken by authors also vary considerably:
some employ rule-based systems, while others choose heavily probabilistic models such as conditional
random fields. In certain cases, conversation reconstruction has been an explored as an evaluation metric 
for unsupervised models that discover \textit{speech acts} in conversational data.

We identified \textit{forum-style disentanglement} as the task of creating 
graphical parent-child links between posts under a related topic. The primary characteristic 
of \textit{forum-style} problems are longer, well formed posts 
linked by relatively unsubjective ground truth links. In an early
attempt at this problem domain, \cite{Wang2008a} address reconstructing
thread structure using message similarity alone. They employ a rule-based
system that creates graphical links between messages to represent a connection.
They employ a small (478) message corpus taken from a political science domain. 
Peculiarly, the authors manually created their own ground truth links and ended
up without an edge-wise F1 score of 0.40.

Unlike the rule-based system of \cite{Wang2008a}, the authors of 
in \cite{Wang2011a} employ machine learning to aid in their task. They employ
the machinery of conditional random fields for link prediction.
%This seems like a candidate to put something else into
The the authors defines a conditional distribution over the reply
relationship given a post sequence and formulates it as a Maximum a Posteriori
(MAP) problem. A crucial component of this task is defining a set of features
that capture dependencies between posts. The paper describes two categories of
features. The first kind depend only on attributes drawn from a pair
of posts, such as content similarity score, recency, author name reference, and
indicator variables that give special importance to posts made last or first in
a given thread. The second kind also consider the parent assignments of
the posts, which allows scoring of characteristics like the
similarity between  post and the \emph{parent} of another post to determine
content propagation, or accord importance to posts that reply to the same author
and are also written by the same author (which is indicative of friends carrying
out a back-and-forth conversation in a thread). The weights for their full set of
13 features is trained through a supervised classifier that maximizes the
conditional likelihood for the reply sequence.

Similarly, in \cite{Aumayr2011a}, the authors attempt machine learning to 
tackle the problem of reconstructing conversation trees in a traditional
vBulletin-powered online forum. This assumes that there was thread-structure 
in the original data. The authors report their best results using C4.5-style
decision trees based on classifying whether pairs of posts are in a $(parent, child)$
relationship or not. Their features are a mix of shallow textual and post 
metadata features: TF-IDF weighted text distance, 
presence of quotes and a user's name, post-distance and time difference. 
Although post distance is their best feature, is particularly suited to the 
near-linear structure of their domain.

We identify the \textit{chat disentanglement} problem as grouping
shorter conversational-style messages into coherent sub-conversations.
In contrast to forum-style disentanglement, the length of each messages tend to be shorter
and ground truth data is quite subjective (what constitutes a sub-conversation?).

In early work, \ref{Elsner2008a} treat disentanglement as a clustering problem between 
utterances. Their dataset consists of manually annotated IRC conversations that contain
information about schisms in topics and minor digressions. Similar to most authors, 
they employ a machine-learning model using a MaxEnt classifier to decide whether a given pair of
utterances belongs to the same conversation. It then attempts to cluster the
utterances to maximize the weighted accuracy of the classifier. As with much other
work, they employ pause length and message similarity as key features in
their classifier.

\ref{Wang2009b} expands on the above study by distinguishing
\textit{context-free} and \textit{context-sensitive} message models, where the 
former refers to using data in the message alone, including word features and the
time-stamp.  This study augments a basic set of word features with author context, 
conversational context, and temporal context of a message. Author context 
refers to all other messages written by the author, and conversational context 
refers to all messages by participants in a sub-conversation, 
where the participants are determined by all other people referred to by name 
by the author of a message.  Afterward computing their features, they perform
clustering to identify different threads of conversation.

A very different approach is seen in \cite{Mayfield2012a}, the authors present
a model for chat disentanglement that annotates three types of information: 1) 
Negotiation labels, or whether an utterance is providing or receiving 
information, 2) sequences, or exchanges of information, and 3) threads, the 
outcome of the standard disentanglement problem. Their algorithm performs 
two passes over the data,  one in which negotiations and sequences are generated, 
followed by a pass that groups sequences together to form threads. Uniquely,
their data was hand-annotated from verbal chat meetings for a cancer support group.
They report results that occasionally beat human annotators.

Finally, some authors consider the problem of speech disentanglement as traversing 
a finite state automaton between different \textit{speech acts}, such as
``question'', ``answer'' or ``statement''. Although this is a classic problem in
natural language processing, it is natural evaluate their success extrinsically 
using discourse disentanglement. The authors of \cite{Paula} build an 
unsupervised Hidden Markov Model that  maps speech acts to hidden states. 
They believe that the words of a message 
in a conversation are generated according to language models associated
with a state or speech act. They report hidden states
that correfctly correspond to speech acts, for example, a collection of
question-related pronouns and words corresponding to a speech act beginning a
forum post about a technical question. 

The authors of \cite{Ritter2010a} consider unsupervised tagging of speech acts.
The paper describes an unsupervised LDA-like approach to act tagging 
and utilizes Twitter \textit{conversations} for
training purposes. Fittingly, their model was evaluated 
extrinsically by reconstructing the original order of a scrambled
conversations in Twitter. Their model corresponds to a generative story
where words are filled by either the conversation topic, the dialog
act of the tweet or from general English.

Unlike other speech act papers, \cite{Kim2012} 
focuses on classifying utterances
in multi-party chat scenarios. The basic unit
of analysis is a contiguous message written by an individual. The
experiment classifies messages into discourse acts such as a greeting, yes/no
question, request for information, or 11 other categories defined in previous
works, with two new ones introduced here.

The authors define several classes of features, then runs various classifiers on
combinations of them. The features include TF-IDF and boolean unigrams, with
and without stemming and lemmatization.  The best
performance in both datasets approaches 100\% on a model using bag-of-words,
keywords, and previous utterances, but not structural information. 
The authors conclude that the failure of structural information 
compared to previous works is likely due to entanglement.

\subsection{Dataset}
\label{section:dataset}
Our approach differs from most of those listed in Section \ref{sec:approaches}
by the scale of our approach: we are trying to reconstruct complex
conversations with large branching numbers.  In that vein, we extract data
from Reddit, an online link-sharing and discussion site. To ensure all
conversations have a root element, we used the popular AskReddit subforum
where users initiate topics of conversation with a question. We treat the
initial post as the root of a conversation tree, and each top-level comment as
a child of the root. Each message node is annotated with various data such as
timestamp, username, and more.

To grab the data, we wrote a quick JRuby crawler and took a small sample (342)
of the top AskReddit posts over the last month. The crawler was written to
make data gathering easy and we anticipate creating a bigger dataset to report
our final results. Table \ref{table:stats} shows some aggregated statistics of
the preliminary crawl. In particular, we have an average of 200 messages per
tree and a branching factor within a rounding error of 1.0. The root node has
an average branching factor of 55.50, suggesting that discussions are wide
while sub-discussions tend to be narrow. It is also important to note that
standard deviation for many of our statistics are often quite large relative
to the value, suggesting the wide variance in conversations on Reddit.

\begin{table}[h]\footnotesize
 \begin{tabular}{| l | l | l |} 
   \hline
   \textbf{Statistic} & \textbf{$n / \mu$} & \textbf{$\sigma$} \\
   \hline
    \# Conversation Trees &  342 & \\
    \# Conversation Nodes (Posts) & 68639 & \\
    \# Nodes Per Tree &  200.70 & 200.20 \\
    Mean Branching Factor & 1.00 & 4.20 \\
    Mean Root Branching Factor &  55.50 & 57.91 \\
    Mean Node Depth  & 3.56 & 3.40 \\
    Mean Parent Lag & 7767.96s & 11331.07s \\
    Mean Body Length & 200.43 c & 483.07 \\
   \hline
  \end{tabular}
  \caption{Data statistics of our preliminary conversation dataset}
  \label{table:stats}
\end{table}

We have divided the input data randomly into training / development / test 
sets at a 70\% / 10\% / 20\% split.

\section{Our Model}
\subsection{Approach}
\label{sec:approach}
We are approaching conversation reconstruction as a \textit{classification} problem
between parent and children messages. We take a threaded conversations from a modern internet forum 
(described in Section~\ref{section:dataset}), flatten them into a time-sorted list, and them feed
them into our reconstruction \textit{classifier} that attempts to recreate the tree
with only message-level information. The details our reconstruction algorithm are 
described in Section \ref{sec:classifier}.

\subsection{Evaluation}
For now, we are primarily considering F1 score as our evaluation metric. We
compute precision and recall based on how many of our guessed 
$(parent,child)$ conversation edges match the gold tree. We are considering 
two versions of F1: one were precision and recall is calculated pairwise 
across all messages in all conversations, and another where F1 is calculated 
per-conversation, and the scores are averaged in the end. We expect to have 
very poor performance  in large conversations, so we expect the average F1
score metric to be more forgiving. Both metrics have been implemented in our 
code. Since our crawled dataset currently contains conversations of similar 
size, our distinct evaluation metrics result in scores that are equal to the 
4th decimal. We anticipate our final dataset will have more diversity.

Noting that it can be more important to get lower-depth subtrees correct, we
are also considering weighting a clustering metric such as $B^3$. Here, we 
can treat subtrees at a given depth as clusters, comparing against gold 
clusters at that depth. We could weight proportional to the inverse depth to 
reward us for capturing more of the general discourse with less sensitivity 
to individual placement.

\section{Results}
\label{sec:classifier}
However, our excitement at crossing the trivial baseline should be limited,
since we don't currently have a visualization of the results, and it's
possible that the SVM is still making random guesses, leading to 32\% F1 by
simply associating all messages with the root, for example. This is something
we still need to address by carefully visualizing the actual predicted trees.

\begin{table}[h]\footnotesize
 \begin{tabular}{| l | l | l |} 
   \hline
   \textbf{Statistic} & Pairwise F1 & Average-tree F1 \\
   \hline
    SVM baseline & 0.329 & 0.331 \\
    All-reply-to-root baseline & 0.278 & 0.278 \\
    One-thread baseline & 0.021 & 0.021 \\
   \hline
  \end{tabular}
  \caption{Data statistics of our preliminary conversation dataset}
  \label{table:results}
\end{table}

\section{Error Analysis}
%TODO
We should put something here.

\section{Conclusion and Next Steps}
%TODO
We should also put something here


\bibliography{final_report}{} 
\bibliographystyle{acl2012}

\end{document}
